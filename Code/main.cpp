  #include <sys/types.h>
  #include <sys/stat.h>
  #include <string>
  #include <fstream>
  #include <iostream>
  #include <unistd.h>
  #include <thread>

  //====MRAA includes
  #include "mraa.hpp"
  #include <csignal>  //Library from the C/++ standard libraries to allow for clean exits.
  #include <cstdlib>  //
  //====

  #include "opencv2/imgproc/types_c.h"
 

  #include <librealsense/rs.hpp>
  #include <signal.h>
  #include "rs_sdk.h"
 // #include "or_configuration_interface.h"
 // #include "or_data_interface.h"
 // #include "or_interface.h"
 
#include "ObstacleManager.h"
#include "Object_Recognition.h"
#include "AudioManager.h"
#include "NavigationUtils.h"



  //mraa
  mraa::Gpio *gpio;

  const char COLOR_WINDOW_NAME[] = "Collision Avoidance";
  const char DEPTH_WINDOW_NAME[] = "Depth View";
  

  enum PinNumbers{C=16, R=18,L=20};

  /****create and configure device*******************************/
  rs::core::status initDevice();

  /****checks if theFre might be a collision**********************/

  void voiceORObject(std::string object_name);
  void rotateDir(int collision);
  int* directionCol(cv::Mat depth);
  rs::core::correlated_sample_set* GetSampleSet();
  /****get next frame - color and depth**************************/
  int GetNextFrame(rs::device* device,rs::core::correlated_sample_set& sample_set);
  /****create cv mat with RGB format from image_interface********/
  cv::Mat Image2Mat(rs::core::image_interface* image);
  int8_t get_pixel_size(rs::format format);
  rs::core::status initOR();
  void audioMessage(NavigationUtils::Constants::Direction dir);

  
  void rotate(int pinNum, int length);
  void signal_handler(int sig);


  
  rs::core::status st;

  rs::core::video_module_interface::actual_module_config actualModuleConfig;

  //================OR Variables===============

  bool orInitilized=false;
  //===========================================

   
  ORUtils::ObjectRecognizer objRecognizer;
  ObstacleUtils::ObstacleDetector obsDetector;
  NavigationUtils::Navigator navigator;
  
  bool isGesture;
  bool isObjectFound;

  bool play = true;
  bool playback = true;
  char * playback_file_name;
  int m_frame_number = 0;

  void my_handler(int s)
  {
      if (play)
      {
	  play = false;
      }
      else
      {
	  exit(1);
      }
  }

  //This function is called a "signal handler." It allows a signal like "SIGINT" to exit the program cleanly.
  //In this case, the SIGINT will be generated by the OS when you press Ctrl+C on your keyboard.
  void signal_handler(int sig) {
	  delete gpio;
	  std::cout << "Exiting." << std::endl;
	  exit(0);
  }
  
  void rotate(int pinNum, int length, bool flag){
  //signal(SIGINT, signal_handler); //This sets the event handler to the SIGINT signal. This is the signal generated by Ctrl+c.

	  //std::cout << "Hello from Intel on Joule!" <<std::endl;	//Remember, c++ isn't whitespace-sensitive, so you can use "carriage returns" (split lines) in the middle of function calls.
	  //<< "Press Ctrl+c to exit..." << std::endl;				//This is very useful when the function call will be long and otherwise unwieldy.
	  std::cout<<pinNum<<std::endl;

	  gpio = new mraa::Gpio(pinNum);	//Instantiate the GPIO object so that we can access the pin. (Pin 100)

  //12 - center,14 - left,16 -right
	  if (gpio) { //If the instantiation was successful...
		  gpio->dir(mraa::DIR_OUT); //Set the pin as an output.
		  
		  //while (true) { //Begin an infinite loop. This will exit on pressing Ctrl+c, due to the event handler.
			  gpio->write(flag); //gpio->write(n) writes the value "n" to the gpio object. If the value is 1, the output turns off, if it's 0 it turns on. This may seem counterintuitive, but it's called active-low.
			  //sleep(1000/length);		//sleep(n) is exactly what it says on the tin. It makes the program (or thread) sleep for n seconds.
			  //gpio->write(0);
			  //sleep(1);
		  //}
	  }
  }
  
  int main(int argc, char** argv)
  {
    std::cout<<"hi hi hi"<<std::endl;
    
    int width =628/3, height=468/4;
    cv::Rect rec_left(0,height,width,height*3), rec_right(width*2,height,width,height*3),rec_center(width,height,width,height*3);
    
    int width_rgb =1920/3, height_rgb=1080/4;
    cv::Rect rec_left_rgb(0,height_rgb,width_rgb,height_rgb*3), rec_right_rgb(width_rgb*2,height_rgb,width_rgb,height_rgb*3),rec_center_rgb(width_rgb,height_rgb,width_rgb,height_rgb*3);
    
    /*objRecognizer = ORUtils::ObjectRecognizer();
    obsDetector = ObstacleUtils::ObstacleDetector();
    navigator = NavigationUtils::Navigator();*/
        
    system("gst-play-1.0 welcome.wav &");
    
      /**********************handle ctrl-c - for proper closing of the camera***********************/
      struct sigaction sigIntHandler;
      sigIntHandler.sa_handler = my_handler;
      sigemptyset(&sigIntHandler.sa_mask);
      sigIntHandler.sa_flags = 0;
      sigaction(SIGINT, &sigIntHandler, NULL);

      /***********************************initialize device*****************************************/
      if (argc < 2)
      {
	  std::cerr<<"Mode is: Record" << std::endl;
	  playback = false;
      }
      else
      {
	if(access(argv[1], F_OK) == -1)
	{    
	  std::cerr << "playback file does not exists" << std::endl;
	  return -1;
        }
	std::cerr<<"Mode is: Playback" << std::endl;
	playback_file_name = argv[1];  
	
      }

      //create a playback enabled context with a given output file
      //rs::playback::context context(playback_file_name.c_str());
      rs::core::status st;
      st = objRecognizer.initDevice();
      st = objRecognizer.initOR();
      
      if(st == rs::core::status_no_error)
      {
	orInitilized = true;
	//objRecognition.startCamera();
      }
	
      /***********************************configure GUI*********************************************/
      cv::namedWindow(COLOR_WINDOW_NAME, CV_WINDOW_NORMAL);
      cv::resizeWindow(COLOR_WINDOW_NAME, 480, 270);
      cv::moveWindow(COLOR_WINDOW_NAME,200,200);
      cv::namedWindow(DEPTH_WINDOW_NAME, CV_WINDOW_NORMAL);
      cv::resizeWindow(DEPTH_WINDOW_NAME, 640, 360);
      cv::moveWindow(DEPTH_WINDOW_NAME,700,200);
      cv::startWindowThread();

      /***********************************start streaming*******************************************/
      while(play && objRecognizer.device->is_streaming())
      {	  
	  
          rs::core::correlated_sample_set sampleSet;

          if (GetNextFrame(objRecognizer.device, sampleSet) != 0)
          {
              std::cout<<"invalid frame"<<std::endl;
              continue;
          }

          cv::Mat renderImage = Image2Mat(sampleSet[rs::core::stream_type::color]);
          cv::Mat depthMat = Image2Mat(sampleSet[rs::core::stream_type::depth]);

          int* col = NULL;

          if(!depthMat.empty())
          {
            if( m_frame_number%10 == 0 )
            {
              col = obsDetector.directionCol(depthMat);//+++++++++++++++++++++
              navigator.navigate(col);

              cv::rectangle(depthMat,rec_left,cv::Scalar(255,0,0),6);//draw left rect
              if((col[0] & ObstacleUtils::Obstacle::LEFT)!=0)
            cv::putText(renderImage,"BLOCKED!", cv::Point(300,100),2,1.0,cv::Scalar(0,255,255));//draw right rect

              cv::rectangle(depthMat,rec_right,cv::Scalar(255,0,0),6);
              if((col[0] & ObstacleUtils::Obstacle::RIGHT)!=0)
            cv::putText(renderImage,"BLOCKED!", cv::Point(1500,100),2,1.0,cv::Scalar(0,255,255));

              cv::rectangle(depthMat,rec_center,cv::Scalar(255,0,0),6);//draw center rect
              if((col[0] & ObstacleUtils::Obstacle::CENTER)!=0)
            cv::putText(renderImage,"BLOCKED!", cv::Point( 1000,100),2,1.0,cv::Scalar(0,255,255));

              cv::rectangle(renderImage,rec_left_rgb,cv::Scalar(255,0,0),4);
              cv::rectangle(renderImage,rec_right_rgb,cv::Scalar(255,0,0),4);
              cv::rectangle(renderImage,rec_center_rgb,cv::Scalar(255,0,0),4);
            }

            /*if (( m_frame_number%50 == 0)&& orInitilized &&col[2]!=0)//process OR
                      {
                              if(objRecognition.set_rect(col[2])!= rs::core::status_no_error)
                      {
                        std::cout<<"failed to set roi"<<std::endl;
                      }
                      else
                      {
                        objRecognition.process_sample(sampleSet);
                        std::string or_object_name = objRecognition.get_object_name();
                        if(or_object_name != "")
                          std::cout<<"or object name : "<<or_object_name<<std::endl;
                      }


                      }*/

          }

          cv::imshow(COLOR_WINDOW_NAME, renderImage);
          cv::imshow(DEPTH_WINDOW_NAME, depthMat*200);

          //get frame
          m_frame_number++;
      }
      objRecognizer.device->stop();
      //delete gpio;
      std::printf("exiting\n");
  }
  

  bool startedC=false,startedL=false,startedR=false;
  


  
  /*void audioMessage(NavigationUtils::Constants::Direction d)
  {
    std::cout<<"direction to turn to: " << d << std::endl;
    if(d == NavigationUtils::Constants::Direction::D_RIGHT)
      audioMng.play(AudioManager::MOVE_RIGHT, m_frame_number);
      //      system("gst-play-1.0 move_right.wav &");
    else if(d ==NavigationUtils::Constants::Direction::D_LEFT)
      audioMng.play(AudioManager::MOVE_LEFT,m_frame_number);
      //system("gst-play-1.0 move_left.wav &");
    else if(d == NavigationUtils::Constants::Direction::STOP)
      audioMng.play(AudioManager::STOP,m_frame_number);    
      //system("gst-play-1.0 stop.wav &");
  }*/
  
  void rotateDir(int collision)
  {

    std::cout<<"in rotate"<<std::endl;
    std::cout<< "layer 0: "<<collision<<std::endl;
    
    if((collision & ObstacleUtils::Obstacle::CENTER)!=0)
    {
      std::cout<<"Obstacle in Center"<<std::endl;//obstacle in center
      if(!startedC)
	{
	  startedC=true;
          rotate(PinNumbers::C, 1,startedC);
	}
    }
    else					//center is free
    {
	startedC=false;
	rotate(PinNumbers::C, 1,startedC);
    }
  
    if((collision & ObstacleUtils::Obstacle::LEFT)!=0)
    {
      std::cout<<"Obstacle in Left"<<std::endl;//obstacle in left
      if(!startedL)
      {
	startedL=true;
	rotate(PinNumbers::L, 1,startedL);
      }
    }
    else
    {
      startedL=false;
      rotate(PinNumbers::L, 1,startedL);
    }
    
    if((collision & ObstacleUtils::Obstacle::RIGHT)!=0)
    {
      std::cout<<"Obstacle in Right"<<std::endl;//obstacle in right 
      if(!startedR)
      {
	startedR=true;
	rotate(PinNumbers::L, 1,startedR);
      }
    }
    else
    {
      startedR=false;
      rotate(PinNumbers::L, 1,startedR);
    }
  }
  void  voiceORObject(std::string object_name)
  {
    if (object_name=="bag")
    {
      std::cout<<"There is a "<<object_name<<std::endl;//should be in voice
      system("gst-play-1.0 bag.wav &");
    }
    if (object_name=="bed")
    {
      std::cout<<"There is a "<<object_name<<std::endl;//should be in voice
       system("gst-play-1.0 bed.wav &");
    }
    if (object_name=="cabinet")
    {
      std::cout<<"There is a "<<object_name<<std::endl;//should be in voice
       system("gst-play-1.0 cabinet.wav &");
    }
    if (object_name=="cat")
    {
      std::cout<<"There is a "<<object_name<<std::endl;//should be in voice
       system("gst-play-1.0 cat.wav &");
    }
    if (object_name=="dog")
    {
      std::cout<<"There is a "<<object_name<<std::endl;//should be in voice
       system("gst-play-1.0 dog.wav &");
    }
    if (object_name=="chair")
    {
      std::cout<<"There is a "<<object_name<<std::endl;//should be in voice
       system("gst-play-1.0 chair.wav &");
    }
    if (object_name=="open_door")
    {
      std::cout<<"There is a "<<object_name<<std::endl;//should be in voice
       system("gst-play-1.0 opened_door.wav &");
    } 
  }

  void release_images()
  { /*
      if (m_sample_set->images[(int)rs::stream::color])
      {
	  m_sample_set->images[(int)rs::stream::color]->release();
	  m_sample_set->images[(int)rs::stream::color]=nullptr;
      }

      if (m_sample_set->images[(int)rs::stream::depth])
      {
	  m_sample_set->images[(int)rs::stream::depth]->release();
	  m_sample_set->images[(int)rs::stream::depth] = nullptr;
      }*/
  }
  /*rs::core::status GetSampleSet()
  {
      device->wait_for_frames();

      //get color and depth buffers
      const void* colorBuffer =  device->get_frame_data(rs::stream::color);
      const void* depthBuffer = device->get_frame_data(rs::stream::depth);
      m_color_buffer = const_cast<void*>(colorBuffer);

      // release images from the prevoius frame
      release_images();
      //create images from buffers
      auto colorImg = rs::core::image_interface::create_instance_from_raw_data( &colorInfo, colorBuffer, rs::core::stream_type::color, rs::core::image_interface::any,m_frame_number, (uint64_t)device->get_frame_timestamp(rs::stream::color), nullptr);
      auto depthImg = rs::core::image_interface::create_instance_from_raw_data( &depthInfo, depthBuffer, rs::core::stream_type::depth, rs::core::image_interface::any,m_frame_number, (uint64_t)device->get_frame_timestamp(rs::stream::depth), nullptr);

      //create sample from both images

      m_sample_set->images[(int)rs::stream::color] = colorImg;
      m_sample_set->images[(int)rs::stream::depth] = depthImg;
    

      return rs::core::status_no_error;
  }*/ 



  int GetNextFrame(rs::device* device, rs::core::correlated_sample_set& sample_set)
  {
      /******************************get next frame - color and depth*******************************/
      device->wait_for_frames();

      for(auto &stream :
	      {
		  rs::core::stream_type::color,  rs::core::stream_type::depth
		//rs::core::stream_type::fisheye,  rs::core::stream_type::depth
	      })
      {
	  rs::stream librealsense_stream = rs::utils::convert_stream_type(stream);
	  int height = device->get_stream_height(librealsense_stream);
	  int width = device->get_stream_width(librealsense_stream);
	  rs::core::image_info info = { width,
					height,
					rs::utils::convert_pixel_format(device->get_stream_format(librealsense_stream)),
					get_pixel_size(device->get_stream_format(librealsense_stream)) * width
				      };

	  std::unique_ptr<rs::core::metadata_interface> metadata(rs::core::metadata_interface::create_instance());
	  const void* frameData = device->get_frame_data(librealsense_stream);
	  if (!frameData)
	  {
	      printf("frame data for %d is null\n", (int32_t)librealsense_stream);
	      return -1;
	  }
	  sample_set[stream] = rs::core::image_interface::create_instance_from_raw_data(
				  &info,
				  frameData,
				  stream,
				  rs::core::image_interface::flag::any,
				  device->get_frame_timestamp(librealsense_stream),
				  device->get_frame_number(librealsense_stream),
				  metadata.get());
      }
      return 0;
  }

  cv::Mat Image2Mat(rs::core::image_interface* image)
  {
      /********************create cv mat with RGB format from image_interface***********************/
      cv::Mat mat;
      switch (image->query_info().format)
      {
      case rs::core::pixel_format::rgba8:
	  mat = cv::Mat(image->query_info().height, image->query_info().width, CV_8UC4, (void*)(image->query_data())).clone();
	  cv::cvtColor(mat, mat, CV_RGBA2BGR);
	  break;
      case rs::core::pixel_format::bgra8:
	  mat = cv::Mat(image->query_info().height, image->query_info().width, CV_8UC4, (void*)(image->query_data())).clone();
	  cv::cvtColor(mat, mat, CV_BGRA2BGR);
	  break;
      case rs::core::pixel_format::bgr8:
	  mat = cv::Mat(image->query_info().height, image->query_info().width, CV_8UC3, (void*)(image->query_data())).clone();
	  break;
      case rs::core::pixel_format::rgb8:
	  mat = cv::Mat(image->query_info().height, image->query_info().width, CV_8UC3, (void*)(image->query_data())).clone();
	  cv::cvtColor(mat, mat, CV_RGB2BGR);
	  break;
      case rs::core::pixel_format::z16: //depth image
	  mat = cv::Mat(image->query_info().height, image->query_info().width, CV_16UC1, (void*)(image->query_data())).clone();
	  break;
      default:
	  std::runtime_error("unsupported color format");
      }
      return  mat;
  }

  int8_t get_pixel_size(rs::format format)
  {
      switch(format)
      {
      case rs::format::any:
	  return 0;
      case rs::format::z16:
	  return 2;
      case rs::format::disparity16:
	  return 2;
      case rs::format::xyz32f:
	  return 4;
      case rs::format::yuyv:
	  return 2;
      case rs::format::rgb8:
	  return 3;
      case rs::format::bgr8:
	  return 3;
      case rs::format::rgba8:
	  return 4;
      case rs::format::bgra8:
	  return 4;
      case rs::format::y8:
	  return 1;
      case rs::format::y16:
	  return 2;
      case rs::format::raw8:
	  return 1;
      case rs::format::raw10:
	  return 0;//not supported
      case rs::format::raw16:
	  return 2;
      }
  }
  
  //to run after getting name of object's name
  
  /*std::cout<<or_configuration->query_object_name_by_id(recognition_data[0].label)<<" "<<recognition_data[0].probability*100<<"%"<<std::endl;

		 std::stringstream text;
		 int textHeight = std::max(int(renderImage.rows*0.05), 5);
		 return or_configuration->query_object_name_by_id(recognition_data[0].label);
		 text << object_name << ": " << recognition_data[0].probability;
		 voiceORObject(object_name);
		 cv::putText(renderImage, text.str(), cv::Point(int(renderImage.cols*0.05), int(renderImage.rows*0.1)), cv::FONT_HERSHEY_PLAIN, std::max(textHeight / 12.0, 0.5), cvScalar(255, 0, 0), 3, CV_AA);*/		 
		    
