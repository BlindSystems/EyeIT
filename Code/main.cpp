  #include <sys/types.h>
  #include <sys/stat.h>
  #include <string>
  #include <fstream>
  #include <iostream>
  #include <unistd.h>
  #include <thread>

  //====MRAA includes
  #include "mraa.hpp"
  #include <csignal>  //Library from the C/++ standard libraries to allow for clean exits.
  #include <cstdlib>  //
  #include <unistd.h> //
  #include <iostream>
  //====

  #include "opencv2/imgproc/types_c.h"
  #include "opencv2/opencv.hpp"

  #include <librealsense/rs.hpp>
  #include <signal.h>
  #include "rs_sdk.h"
 // #include "or_configuration_interface.h"
 // #include "or_data_interface.h"
 // #include "or_interface.h"
  #include "or_video_module_impl.h"
  #include "or_data_interface.h"
#include "or_configuration_interface.h"

  //mraa
  mraa::Gpio *gpio;

  const char COLOR_WINDOW_NAME[] = "Collision Avoidance";
  const char DEPTH_WINDOW_NAME[] = "Depth View";
  enum Direction{CENTER=0b1, RIGHT=0b10,LEFT=0b100};
  enum PinNumbers{C=16, R=18,L=20};
  /****create and configure device*******************************/
  rs::core::status initDevice();

  /****checks if there might be a collision**********************/
  bool detectCollision(cv::Mat depth);
  void  voiceORObject(std::string object_name);
  bool detectCollision(cv::Mat depth,cv::Rect roi, int startThresh, int endThresh);
  void rotateDir(int * collision);
  int* directionCol(cv::Mat depth);
  rs::core::correlated_sample_set* GetSampleSet();
  /****get next frame - color and depth**************************/
  int GetNextFrame(rs::device* device,rs::core::correlated_sample_set& sample_set);
  /****create cv mat with RGB format from image_interface********/
  cv::Mat Image2Mat(rs::core::image_interface* image);
  int8_t get_pixel_size(rs::format format);
  rs::core::status initOR();
  void voiceNavigation(int * collision);
  void rotate(int pinNum, int length);
  void signal_handler(int sig);

  rs::device * device;
  rs::core::status st;
  std::unique_ptr<rs::core::context_interface> ctx;
  rs::core::video_module_interface::actual_module_config actualModuleConfig;

  //================OR Variables===============
  rs::core::image_info colorInfo,depthInfo;
  rs::object_recognition::or_video_module_impl impl;
  rs::object_recognition::or_data_interface* or_data = nullptr;
  rs::object_recognition::or_configuration_interface* or_configuration = nullptr;
  //rs::core::correlated_sample_set* m_sample_set;
  void* m_color_buffer;
  int m_frame_number;
  int frame_to_process_or = 50;
  rs::object_recognition::recognition_data* recognition_data;
  int array_size;
  bool orInitilized=false;
  //===========================================


  bool isGesture;
  bool isObjectFound;

  bool play = true;
  bool playback = true;
  char * playback_file_name;

  void my_handler(int s)
  {
      if (play)
      {
	  play = false;
      }
      else
      {
	  exit(1);
      }
  }

  //This function is called a "signal handler." It allows a signal like "SIGINT" to exit the program cleanly.
  //In this case, the SIGINT will be generated by the OS when you press Ctrl+C on your keyboard.
  void signal_handler(int sig) {
	  delete gpio;
	  std::cout << "Exiting." << std::endl;
	  exit(0);
  }
  void rotate(int pinNum, int length, bool flag){
  //signal(SIGINT, signal_handler); //This sets the event handler to the SIGINT signal. This is the signal generated by Ctrl+c.

	  //std::cout << "Hello from Intel on Joule!" <<std::endl;	//Remember, c++ isn't whitespace-sensitive, so you can use "carriage returns" (split lines) in the middle of function calls.
	  //<< "Press Ctrl+c to exit..." << std::endl;				//This is very useful when the function call will be long and otherwise unwieldy.
	  std::cout<<pinNum<<std::endl;

	  gpio = new mraa::Gpio(pinNum);	//Instantiate the GPIO object so that we can access the pin. (Pin 100)

  //12 - center,14 - left,16 -right
	  if (gpio) { //If the instantiation was successful...
		  gpio->dir(mraa::DIR_OUT); //Set the pin as an output.
		  
		  //while (true) { //Begin an infinite loop. This will exit on pressing Ctrl+c, due to the event handler.
			  gpio->write(flag); //gpio->write(n) writes the value "n" to the gpio object. If the value is 1, the output turns off, if it's 0 it turns on. This may seem counterintuitive, but it's called active-low.
			  //sleep(1000/length);		//sleep(n) is exactly what it says on the tin. It makes the program (or thread) sleep for n seconds.
			  //gpio->write(0);
			  //sleep(1);
		  //}
	  }
  }
  int main(int argc, char** argv)
  {
     system("gst-play-1.0 welcome.wav &");
      /**********************handle ctrl-c - for proper closing of the camera***********************/
      struct sigaction sigIntHandler;
      sigIntHandler.sa_handler = my_handler;
      sigemptyset(&sigIntHandler.sa_mask);
      sigIntHandler.sa_flags = 0;
      sigaction(SIGINT, &sigIntHandler, NULL);

      /***********************************initialize device*****************************************/
      if (argc < 2)
      {
	  std::cerr<<"Mode is: Record" << std::endl;
	  playback = false;
      }
      else
      {
      if(access(argv[1], F_OK) == -1)
      {    
	  std::cerr << "playback file does not exists" << std::endl;
	  return -1;
      }
	std::cerr<<"Mode is: Playback" << std::endl;
	playback_file_name = argv[1];  
	
      }

      //create a playback enabled context with a given output file
      //rs::playback::context context(playback_file_name.c_str());
      rs::core::status st;
      st = initDevice();
      //st = initOR();

      /***********************************configure GUI*********************************************/
      cv::namedWindow(COLOR_WINDOW_NAME, CV_WINDOW_NORMAL);
      cv::resizeWindow(COLOR_WINDOW_NAME, 480, 270);
      cv::namedWindow(DEPTH_WINDOW_NAME, CV_WINDOW_NORMAL);
      cv::resizeWindow(DEPTH_WINDOW_NAME, 960, 540);
      cv::moveWindow(COLOR_WINDOW_NAME,1000,200);
      cv::startWindowThread();

      /***********************************start streaming*******************************************/
      while(play && device->is_streaming())
      {
	  //get frame
	  m_frame_number++;
	  rs::core::correlated_sample_set sampleSet;
	  if (GetNextFrame(device, sampleSet) != 0)
	  {
	      printf("invalid frame\n");
	      continue;
	  }

	  cv::Mat renderImage = Image2Mat(sampleSet[rs::core::stream_type::color]);
	  //cv::Mat renderImage = Image2Mat(sampleSet[rs::core::stream_type::fisheye]);
	  cv::Mat depthMat = Image2Mat(sampleSet[rs::core::stream_type::depth]);
	  

	  if(!depthMat.empty())
	  {
	      /*if(!detectCollision(depthMat))
	      {
		  printf("No Collision Appears!\n");
	      }
	      else
	      {
		  printf("Collision detected - Avoid it!\n");
	      }*/

	      int* col=directionCol(depthMat);
	      rotateDir(col);
	      if (( m_frame_number%frame_to_process_or == 0)&& orInitilized &&col[2]!=0){
		//to recognize objects in case of 2m object only
		//add bounding box to recognition
		if(((col[2] & 0b1)!=0) && ((col[2] & 0b10)!=0) && ((col[2] & 0b100)!=0))
	
		  or_configuration->set_roi(rs::core::rectF32{0,0,1,1});//all image
		else if(((col[2] & 0b1)!=0) && !((col[2] & 0b10)!=0) && !((col[2] & 0b100)!=0)) //center only
		  	  or_configuration->set_roi(rs::core::rectF32{0.25,0,0.2,1});
		else if(!((col[2] & 0b1)!=0) && ((col[2] & 0b10)!=0) && !((col[2] & 0b100)!=0)) //Left only
		or_configuration->set_roi(rs::core::rectF32{0,0,0.25,1});
		else if(!((col[2] & 0b1)!=0) && !((col[2] & 0b10)!=0) && ((col[2] & 0b100)!=0)) //Right only
		or_configuration->set_roi(rs::core::rectF32{0.75,0,0.25,1});
		else if(((col[2] & 0b1)!=0) && ((col[2] & 0b10)!=0) && !((col[2] & 0b100)!=0)) //Left & center
		or_configuration->set_roi(rs::core::rectF32{0,0,0.75,1});
		else if(((col[2] & 0b1)!=0) && ((col[2] & 0b10)!=0) && !((col[2] & 0b100)!=0)) //Right & center
		or_configuration->set_roi(rs::core::rectF32{0.25,0,0.75,1});
		else
		    or_configuration->set_roi(rs::core::rectF32{0.25,0,0.2,1});//center only
	or_configuration->apply_changes();
		
		//int objectID = getORObject();
		   st = impl.process_sample_set_sync(&sampleSet);
		   if (st != rs::core::status_no_error)
			 std::cout<<"error while processing or data"<<std::endl;
		   else
		   {
		     st = or_data->query_single_recognition_result(&recognition_data, array_size);
		     if (st != rs::core::status_no_error)
			    std::cout<<"error while quering or data"<<std::endl;
		     else{
		       if(recognition_data[0].probability>0.8)
		       {
			 std::cout<<or_configuration->query_object_name_by_id(recognition_data[0].label)<<" "<<recognition_data[0].probability*100<<"%"<<std::endl;
			 //add voice recognition
			 //cv::Rect cvroi(0.25*colorInfo.width,0.25*colorInfo.height,0.5*colorInfo.width,0.5*colorInfo.height); 
		 //cv::rectangle(renderImage,cvroi,cv::Scalar(255,0,0),2);
		 std::stringstream text;
		 int textHeight = std::max(int(renderImage.rows*0.05), 5);
		 std::string object_name = or_configuration->query_object_name_by_id(recognition_data[0].label);
		 text << object_name << ": " << recognition_data[0].probability;
		 voiceORObject(object_name);
		 cv::putText(renderImage, text.str(), cv::Point(int(renderImage.cols*0.05), int(renderImage.rows*0.1)), cv::FONT_HERSHEY_PLAIN, std::max(textHeight / 12.0, 0.5), cvScalar(255, 0, 0), 3, CV_AA);
		 
		       }
		     }
		   }
	      }
	      
	  }

	  cv::imshow(COLOR_WINDOW_NAME, renderImage);
	  cv::imshow(DEPTH_WINDOW_NAME, depthMat*200);
      }
      device->stop();
      delete gpio;
      printf("exiting\n");
  }
    bool startedC=false,startedL=false,startedR=false;
  void rotateDir(int* collision)
  {

    std::cout<<"rotate"<<std::endl;
      std::cout<<collision[0]<<" "<<Direction::CENTER<<std::endl;
    if((collision[0] & 0b1)!=0){
      std::cout<<"Obstacle in Center"<<std::endl;//rotate in center
      if(!startedC){
	  startedC=true;
	    rotate(PinNumbers::C, 1,startedC);
	    }
    }
      else{
	startedC=false;
	rotate(PinNumbers::C, 1,startedC);
      }
    
  
    //
  
    if((collision[0] & 0b100)!=0){
      std::cout<<"Obstacle in Left"<<std::endl;//rotate in left
	std::cout<<collision[0]<<" "<<Direction::LEFT<<std::endl;
	if(!startedL){
	  startedL=true;
	rotate(PinNumbers::L, 1,startedL);
	}
    }
	else{
	  startedL=false;
      rotate(PinNumbers::L, 1,startedL);
	}
    
    
    if((collision[0] & 0b100)!=0){
      std::cout<<"Obstacle in Right"<<std::endl;//rotate in right 
	std::cout<<collision[0]<<Direction::RIGHT<<std::endl;
	if(!startedR){
	  startedR=true;
	rotate(PinNumbers::L, 1,startedR);
	}
    }
	else{
	  startedR=false;
      rotate(PinNumbers::L, 1,startedR);
	}
      
    if(((collision[0] & 0b1)!=0) && !((collision[0] & 0b10)!=0) && !((collision[0] & 0b100)!=0))
	  voiceNavigation(collision);
  }
  void  voiceORObject(std::string object_name)
  {
    if (object_name=="bag")
    {
      std::cout<<"There is a "<<object_name<<std::endl;//should be in voice
      system("gst-play-1.0 bag.wav &");
    }
    if (object_name=="bed")
    {
      std::cout<<"There is a "<<object_name<<std::endl;//should be in voice
       system("gst-play-1.0 bed.wav &");
    }
    if (object_name=="cabinet")
    {
      std::cout<<"There is a "<<object_name<<std::endl;//should be in voice
       system("gst-play-1.0 cabinet.wav &");
    }
    if (object_name=="cat")
    {
      std::cout<<"There is a "<<object_name<<std::endl;//should be in voice
       system("gst-play-1.0 cat.wav &");
    }
    if (object_name=="dog")
    {
      std::cout<<"There is a "<<object_name<<std::endl;//should be in voice
       system("gst-play-1.0 dog.wav &");
    }
    if (object_name=="chair")
    {
      std::cout<<"There is a "<<object_name<<std::endl;//should be in voice
       system("gst-play-1.0 chair.wav &");
    }
    if (object_name=="open_door")
    {
      std::cout<<"There is a "<<object_name<<std::endl;//should be in voice
       system("gst-play-1.0 opened_door.wav &");
    } 
  }
  void voiceNavigation(int * collision)
  {
    //====================check level '1' ===================================
    if((collision[1] & 0b10) && (collision[1] & 0b100))//collition on both right and left dir -> no perference
    {
      system("gst-play-1.0 stop.wav &");
      return;
    }
    
    if((collision[1] & 0b10) && !(collision[1] & 0b100))
    {
      std::cout<<"Move to your right"<<std::endl;//should be in voice
      system("gst-play-1.0 moveRight.wav &");
      return;
    }
    if((collision[1] & 0b100) && !(collision[1] & 0b10))
    {
      std::cout<<"Move to your left"<<std::endl;//should be in voice
      system("gst-play-1.0 move_left.wav &");
      
      return;
    }
    
  //====================check level '2' ===================================
    if((collision[2] & 0b10) && !(collision[2] & 0b100))
    {
      std::cout<<"Move to your right"<<std::endl;//should be in voice
      system("gst-play-1.0 moveRight.wav &");
      return;
    }
    if((collision[2] & 0b100) && !(collision[2] & 0b10))
    {
      std::cout<<"Move to your left"<<std::endl;//should be in voice
      system("gst-play-1.0 move_left.wav &");
      return;
    }
    

  }
  rs::core::status initDevice()
  {
      if(!playback)
      {
	  ctx.reset(new rs::core::context);
	  
      }
      else ctx.reset(new rs::playback::context(playback_file_name));
      
      if (ctx == nullptr)
	  return rs::core::status_process_failed;

      int deviceCount = ctx->get_device_count();
      if (deviceCount  == 0)
      {
	  printf("No RealSense device connected.\n\n");
	  return rs::core::status_process_failed;
      }

      //get pointer the the camera
      device = ctx->get_device(0);
      
      /**********************************create and configure device********************************/
      //*ctx.reset(new rs::playback::context("/home/maker/Desktop/project/rssdk.rssdk"));
    // if (ctx->get_device_count() == 0)
      //{
	// printf("Error : cant find devices\n");
	  //exit(EXIT_FAILURE);
      //}

      //device = ctx->get_device(0); //device memory managed by the context 

      //enable color and depth streams
      if(!initOR())
      {
      device->enable_stream(rs::stream::color,rs::preset::best_quality);
      //device->enable_stream(rs::stream::fisheye,rs::preset::best_quality);
      device->enable_stream(rs::stream::depth,rs::preset::best_quality);
      device->start();
      }
      else
	std::cout<<"Failrd to init OR"<<std::endl;
    
  }
  int getORObject(rs::core::rect roi)
  {
    /*
    //or_configuration->set_roi(&roi);
  // or_configuration->apply_changes();
    
    //declare data structure and size for results
    
    //rs::core::correlated_sample_set* sample_set = GetSampleSet();
    
    recognition_data = nullptr;

    
    {
      st = impl.process_sample_set_sync(sample_set);
      if (st != rs::core::status_no_error)
	  return 1;

      //retrieve recognition data from the or_data object
      int array_size;
      st =or_data->query_single_recognition_result(&recognition_data, array_size);
      if (st != rs::core::status_no_error)
	  return 1;
      if(recognition_data[0].probability>0.85)
	return recognition_data[0].label;
    }

  //print out the recognition results
  if (recognition_data)
  {
  //    console_view.draw_results(recognition_data, array_size,or_configuration);
  }
  */
  }

  void release_images()
  { /*
      if (m_sample_set->images[(int)rs::stream::color])
      {
	  m_sample_set->images[(int)rs::stream::color]->release();
	  m_sample_set->images[(int)rs::stream::color]=nullptr;
      }

      if (m_sample_set->images[(int)rs::stream::depth])
      {
	  m_sample_set->images[(int)rs::stream::depth]->release();
	  m_sample_set->images[(int)rs::stream::depth] = nullptr;
      }*/
  }
  /*rs::core::status GetSampleSet()
  {
      device->wait_for_frames();

      //get color and depth buffers
      const void* colorBuffer =  device->get_frame_data(rs::stream::color);
      const void* depthBuffer = device->get_frame_data(rs::stream::depth);
      m_color_buffer = const_cast<void*>(colorBuffer);

      // release images from the prevoius frame
      release_images();
      //create images from buffers
      auto colorImg = rs::core::image_interface::create_instance_from_raw_data( &colorInfo, colorBuffer, rs::core::stream_type::color, rs::core::image_interface::any,m_frame_number, (uint64_t)device->get_frame_timestamp(rs::stream::color), nullptr);
      auto depthImg = rs::core::image_interface::create_instance_from_raw_data( &depthInfo, depthBuffer, rs::core::stream_type::depth, rs::core::image_interface::any,m_frame_number, (uint64_t)device->get_frame_timestamp(rs::stream::depth), nullptr);

      //create sample from both images

      m_sample_set->images[(int)rs::stream::color] = colorImg;
      m_sample_set->images[(int)rs::stream::depth] = depthImg;
    

      return rs::core::status_no_error;
  }*/
  rs::core::status initOR()
  {
    // create or implementaion
	//rs::object_recognition::or_video_module_impl impl;

	//request the first (index 0) supproted module config.
	rs::core::video_module_interface::supported_module_config cfg;
	st =impl.query_supported_module_config(0, cfg);
	if (st != rs::core::status_no_error)
		return st;

	//enables streams according to the supported configuration
	device->enable_stream(rs::stream::color, cfg.image_streams_configs[(int)rs::stream::color].min_size.width,
		                                  cfg.image_streams_configs[(int)rs::stream::color].min_size.height,
		                                  rs::format::bgr8,
		                                  cfg.image_streams_configs[(int)rs::stream::color].minimal_frame_rate);

	device->enable_stream(rs::stream::depth, cfg.image_streams_configs[(int)rs::stream::depth].min_size.width,
		                                  cfg.image_streams_configs[(int)rs::stream::depth].min_size.height,
		                                  rs::format::z16,
		                                  cfg.image_streams_configs[(int)rs::stream::depth].minimal_frame_rate);

	//handling color image info (for later using)
	rs::core::image_info colorInfo ;
	colorInfo.height = cfg.image_streams_configs[(int)rs::stream::color].min_size.height;
	colorInfo.width = cfg.image_streams_configs[(int)rs::stream::color].min_size.width;
	colorInfo.format = rs::core::pixel_format::bgr8;
	colorInfo.pitch = colorInfo.width * 3;
	
	//handling depth image info (for later using)
	rs::core::image_info depthInfo;
	depthInfo.height = cfg.image_streams_configs[(int)rs::stream::depth].min_size.height;
	depthInfo.width = cfg.image_streams_configs[(int)rs::stream::depth].min_size.width;
	depthInfo.format = rs::core::pixel_format::z16;
	depthInfo.pitch = depthInfo.width * 2;

	
	device->start();


	// get the extrisics paramters from the camera
	rs::extrinsics ext  = device->get_extrinsics(rs::stream::depth, rs::stream::color);

	//get color intrinsics
	rs::intrinsics colorInt = device->get_stream_intrinsics(rs::stream::color);

	//get depth intrinsics
	rs::intrinsics depthInt = device->get_stream_intrinsics(rs::stream::depth);

	// after getting all parameters from the camera we need to set the actual_module_config 
	rs::core::video_module_interface::actual_module_config actualConfig;

		//1. copy the extrinsics 
		memcpy(&actualConfig.image_streams_configs[(int)rs::stream::color].extrinsics, &ext, sizeof(rs::extrinsics));

		//2. copy the color intrinsics 
		memcpy(&actualConfig.image_streams_configs[(int)rs::stream::color].intrinsics, &colorInt, sizeof(rs::intrinsics));

		//3. copy the depth intrinsics 
		memcpy(&actualConfig.image_streams_configs[(int)rs::stream::depth].intrinsics, &depthInt, sizeof(rs::intrinsics));

	// handling projection
	//rs::core::projection_interface* proj = rs::core::projection_interface::create_instance(&colorInt, &depthInt, &ext);
	//impl.set_projection(proj);

	//setting the selected configuration (after projection)
	st=impl.set_module_config(actualConfig);
	if (st != rs::core::status_no_error)
		return st;

	//create or data object
	/*rs::object_recognition::or_data_interface* */ or_data = impl.create_output();

	//create or data object
	/*rs::object_recognition::or_configuration_interface* */ or_configuration = impl.create_active_configuration();
	
	//add bounding box to recognition
	//or_configuration->set_roi(rs::core::rectF32{0.25,0.25,0.5,0.5});
	//or_configuration->apply_changes();
	
	//declare data structure and size for results
	/*rs::object_recognition::recognition_data* recognition_data;*/
	/*int array_size;*/
	orInitilized=true;

    /*
      rs::core::status st;
      m_frame_number=0;

      //request the first (index 0) supproted module config.
      rs::core::video_module_interface::supported_module_config cfg;
      st = impl.query_supported_module_config(4, cfg);
      if (st != rs::core::status_no_error)
	  return st;

      //enables streams according to the supported configuration
      device->enable_stream(rs::stream::color, cfg.image_streams_configs[(int)rs::stream::color].min_size.width,
			  cfg.image_streams_configs[(int)rs::stream::color].min_size.height,
			  rs::format::bgr8,
			  cfg.image_streams_configs[(int)rs::stream::color].minimal_frame_rate);

      device->enable_stream(rs::stream::depth, cfg.image_streams_configs[(int)rs::stream::depth].min_size.width,
			  cfg.image_streams_configs[(int)rs::stream::depth].min_size.height,
			  rs::format::z16,
			  cfg.image_streams_configs[(int)rs::stream::depth].minimal_frame_rate);

      //handling color image info (for later using)
      colorInfo.height = cfg.image_streams_configs[(int)rs::stream::color].min_size.height;
      colorInfo.width = cfg.image_streams_configs[(int)rs::stream::color].min_size.width;
      colorInfo.format = rs::core::pixel_format::bgr8;
      colorInfo.pitch = colorInfo.width * 3;

      //handling depth image info (for later using)
      depthInfo.height = cfg.image_streams_configs[(int)rs::stream::depth].min_size.height;
      depthInfo.width = cfg.image_streams_configs[(int)rs::stream::depth].min_size.width;
      depthInfo.format = rs::core::pixel_format::z16;
      depthInfo.pitch = depthInfo.width * 2;
      
      std::cout<<"color :"<<colorInfo.height<<" "<<colorInfo.width<<" "<<(int)colorInfo.format<<" depth "<<depthInfo.height<<" "<<depthInfo.width<<" "<<(int)depthInfo.format<<std::endl;

      device->start();

      //enable auto exposure for color stream
      device->set_option(rs::option::color_enable_auto_exposure, 1);

      //enable auto exposure for Depth camera stream
      device->set_option(rs::option::r200_lr_auto_exposure_enabled, 1);

      // get the extrisics paramters from the camera
      rs::extrinsics ext  = device->get_extrinsics(rs::stream::depth, rs::stream::color);
      rs::core::extrinsics core_ext;

      //get color intrinsics
      rs::intrinsics colorInt = device->get_stream_intrinsics(rs::stream::color);
      rs::core::intrinsics core_colorInt;

      //get depth intrinsics
      rs::intrinsics depthInt = device->get_stream_intrinsics(rs::stream::depth);
      rs::core::intrinsics core_depthInt;

      // after getting all parameters from the camera we need to set the actual_module_config
      rs::core::video_module_interface::actual_module_config actualConfig;

      //1. copy the extrinsics
      memcpy(&actualConfig.image_streams_configs[(int)rs::stream::color].extrinsics, &ext, sizeof(rs::extrinsics));
      core_ext =  rs::utils::convert_extrinsics(ext);

      //2. copy the color intrinsics
      memcpy(&actualConfig.image_streams_configs[(int)rs::stream::color].intrinsics, &colorInt, sizeof(rs::intrinsics));
      core_colorInt = rs::utils::convert_intrinsics (colorInt);

      //3. copy the depth intrinsics
      memcpy(&actualConfig.image_streams_configs[(int)rs::stream::depth].intrinsics, &depthInt, sizeof(rs::intrinsics));
      core_depthInt = rs::utils::convert_intrinsics(depthInt);


      // handling projection
      rs::core::projection_interface* proj = rs::core::projection_interface::create_instance(&core_colorInt, &core_depthInt, &core_ext);
      actualConfig.projection = proj;
      //setting the selected configuration (after projection)
      st=impl.set_module_config(actualConfig);
      if (st != rs::core::status_no_error)
	  return st;

      //create or data object
      or_data = impl.create_output();

      //create or data object
      or_configuration = impl.create_active_configuration();

      m_sample_set = new rs::core::correlated_sample_set();

      m_sample_set->images[(int)rs::stream::color]=nullptr;
      m_sample_set->images[(int)rs::stream::depth]=nullptr;

      return rs::core::status_no_error;
      */
  }

  int* directionCol(cv::Mat depth)
  {
    int* returnCode=new int[3]{0,0,0};
    int width =depth.cols/4, height=depth.rows/4;
    cv::Rect left(0,height,width,height*3), right(width*3,height,width,height*3),center(width,height,width*2,height*3);
    for(int i=0;i<3;i++)
    {
      if(detectCollision(depth,center,i*1000,(i+1)*1000)){
	returnCode[i]|=0b1;
	std::cout<<"collision in center"<<std::endl;
      }
      if(detectCollision(depth,right,i*1000,(i+1)*1000)){
	returnCode[i]|=0b10;
	std::cout<<"collision in right"<<std::endl;
      }
      
      if(detectCollision(depth,left,i*1000,(i+1)*1000)){
	returnCode[i]|=0b100;
      std::cout<<"collision in left"<<std::endl;
	
      }
      std::cout<<returnCode[i]<<std::endl;
    }
      
    return returnCode;
      
  }
  bool detectCollision(cv::Mat depth, cv::Rect roi, int startThresh, int endThresh)//,cv::Rect roi,int startThresh, int endThresh)
  {
      /***************************checks if there might be a collision******************************/
      //rect from 0-80 in the height, and 50-220 in the width
      //cv::Rect roi(50,160,220,80);
      //std::cout<<roi.x<<" "<<roi.y<<" "<<roi.width<<" "<<roi.height<<" "<<std::endl;
  //cv::rectangle(depth,roi,cv::Scalar(255,0,0),3);
  //cv::imshow(COLOR_WINDOW_NAME,depth*200);
  //sleep(500);
  //cv::waitKey(0);
      cv::Mat fov(depth(roi));
      cv::Mat m = fov == 0.0;
      cv::Mat thresh=fov<endThresh & fov>startThresh & fov!=0.0;
      //cv::Mat thresh=fov<startThresh &fov>endThresh &fov!=0.0;

      int close_pixels =cv::countNonZero(thresh);

      if((float)close_pixels/((float)roi.width*roi.height )>0.3)
	  return true;
      int falseData = cv::countNonZero(m);
      if((float)falseData/((float)roi.width*roi.height )>0.7)
	  return true;
      return false;
  }

  int GetNextFrame(rs::device* device, rs::core::correlated_sample_set& sample_set)
  {
      /******************************get next frame - color and depth*******************************/
      device->wait_for_frames();

      for(auto &stream :
	      {
		  rs::core::stream_type::color,  rs::core::stream_type::depth
		//rs::core::stream_type::fisheye,  rs::core::stream_type::depth
	      })
      {
	  rs::stream librealsense_stream = rs::utils::convert_stream_type(stream);
	  int height = device->get_stream_height(librealsense_stream);
	  int width = device->get_stream_width(librealsense_stream);
	  rs::core::image_info info = { width,
					height,
					rs::utils::convert_pixel_format(device->get_stream_format(librealsense_stream)),
					get_pixel_size(device->get_stream_format(librealsense_stream)) * width
				      };

	  std::unique_ptr<rs::core::metadata_interface> metadata(rs::core::metadata_interface::create_instance());
	  const void* frameData = device->get_frame_data(librealsense_stream);
	  if (!frameData)
	  {
	      printf("frame data for %d is null\n", (int32_t)librealsense_stream);
	      return -1;
	  }
	  sample_set[stream] = rs::core::image_interface::create_instance_from_raw_data(
				  &info,
				  frameData,
				  stream,
				  rs::core::image_interface::flag::any,
				  device->get_frame_timestamp(librealsense_stream),
				  device->get_frame_number(librealsense_stream),
				  metadata.get());
      }
      return 0;
  }

  cv::Mat Image2Mat(rs::core::image_interface* image)
  {
      /********************create cv mat with RGB format from image_interface***********************/
      cv::Mat mat;
      switch (image->query_info().format)
      {
      case rs::core::pixel_format::rgba8:
	  mat = cv::Mat(image->query_info().height, image->query_info().width, CV_8UC4, (void*)(image->query_data())).clone();
	  cv::cvtColor(mat, mat, CV_RGBA2BGR);
	  break;
      case rs::core::pixel_format::bgra8:
	  mat = cv::Mat(image->query_info().height, image->query_info().width, CV_8UC4, (void*)(image->query_data())).clone();
	  cv::cvtColor(mat, mat, CV_BGRA2BGR);
	  break;
      case rs::core::pixel_format::bgr8:
	  mat = cv::Mat(image->query_info().height, image->query_info().width, CV_8UC3, (void*)(image->query_data())).clone();
	  break;
      case rs::core::pixel_format::rgb8:
	  mat = cv::Mat(image->query_info().height, image->query_info().width, CV_8UC3, (void*)(image->query_data())).clone();
	  cv::cvtColor(mat, mat, CV_RGB2BGR);
	  break;
      case rs::core::pixel_format::z16: //depth image
	  mat = cv::Mat(image->query_info().height, image->query_info().width, CV_16UC1, (void*)(image->query_data())).clone();
	  break;
      default:
	  std::runtime_error("unsupported color format");
      }
      return  mat;
  }

  int8_t get_pixel_size(rs::format format)
  {
      switch(format)
      {
      case rs::format::any:
	  return 0;
      case rs::format::z16:
	  return 2;
      case rs::format::disparity16:
	  return 2;
      case rs::format::xyz32f:
	  return 4;
      case rs::format::yuyv:
	  return 2;
      case rs::format::rgb8:
	  return 3;
      case rs::format::bgr8:
	  return 3;
      case rs::format::rgba8:
	  return 4;
      case rs::format::bgra8:
	  return 4;
      case rs::format::y8:
	  return 1;
      case rs::format::y16:
	  return 2;
      case rs::format::raw8:
	  return 1;
      case rs::format::raw10:
	  return 0;//not supported
      case rs::format::raw16:
	  return 2;
      }
  }

